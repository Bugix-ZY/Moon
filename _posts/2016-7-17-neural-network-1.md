---
layout: post
title: "Neural Network (I)"
date: 2016-07-17
excerpt: "æ„ŸçŸ¥å™¨ã€æ¿€åŠ±ç¥ç»å…ƒã€åŸºæœ¬ç¥ç»ç½‘ç»œç»“æ„ã€æ¢¯åº¦ä¸‹é™"
tags: [Deep Learning, Machine Learning]
comments: true
---

å‚è€ƒèµ„æ–™:
[Using neural nets to recognize handwritten digits--Michael Nielsen ](http://neuralnetworksanddeeplearning.com/chap1.html) 

*****

### 1. perceptronsæ„ŸçŸ¥å™¨

#### æ„ŸçŸ¥å™¨ ï¼š
(æš‚ä¸”ç§°ä¸ºä¸€ä¸ªåšå†³ç­–çš„å°èŠ‚ç‚¹)è¾“å…¥x1,x2, .., xnï¼Œå¼•å…¥æƒé‡weightsã€‚åŠ æƒä¹‹åå’Œthresholdåšæ¯”è¾ƒï¼Œç„¶åè¾“å‡ºä¸€ä¸ªç»“æœã€‚
##### å¦‚ä¸‹å›¾ï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/single-perceptron.png?raw=true" height="103" width="210" />

##### perceptron rule:

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/threshold.png?raw=true" height="93" width="400">

#### å¤šå±‚æ„ŸçŸ¥å™¨ï¼š
å°±æ˜¯ä¸€å±‚å¤šä¸ªã€ç”±å¤šå±‚çš„æ„ŸçŸ¥å™¨ä¸€èµ·æ¥åšå†³ç­–ã€‚ç¬¬lï¼1å±‚çš„æ„ŸçŸ¥å™¨ä»¬çš„è¾“å‡ºä½œä¸ºç¬¬lå±‚çš„æ„ŸçŸ¥å™¨ä»¬çš„è¾“å…¥ã€‚æ¯å±‚éƒ½å»ºç«‹åœ¨å‰ä¸€å±‚çš„åŸºç¡€ä¸Šï¼Œæ‰€ä»¥å¯ä»¥åšæ›´å¤æ‚çš„å†³ç­–ã€‚
##### å¦‚ä¸‹å›¾ï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/many-layer-network-of-perceptron.png?raw=true" height="141" width="360">

å…³äºè¿™å¼ å›¾æœ‰ä¸€ç‚¹è¦è¯´æ˜ã€‚ç¬¬ä¸€å±‚çš„æ„ŸçŸ¥å™¨ä»¬çœ‹ä¼¼æ˜¯å››ä¸ªç®­å¤´æŒ‡å‘ä¸‹ä¸€å±‚åƒæ˜¯æœ‰å››ä¸ªè¾“å‡ºï¼Œå®é™…ä¸Šå®ƒä»¬ä¾æ—§æ˜¯å•ä¸ªè¾“å‡ºï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†è¡¨æ˜å®ƒä»¬çš„å•ä¸ªè¾“å‡ºä¼šä½œä¸ºä¸‹ä¸€å±‚æ„ŸçŸ¥å™¨ä»¬çš„è¾“å…¥ã€‚


ä¸ºäº†åç»­æ•°å­¦ç¬¦å·ä¸Šçš„ä¾¿æ·ï¼Œåšäº›ç»†å¾®çš„è°ƒæ•´ã€‚
è®¤ä¸ºw,xåˆ†åˆ«æ˜¯æƒé‡å‘é‡å’Œè¾“å…¥å‘é‡, å°†ä¹‹å‰æåˆ°çš„thresholdæŒªä¸ªä½ç½®å¹¶ä¸”ç§°å…¶ä¸ºbiasã€‚è¿™æ ·å°±å¾—åˆ°äº†å¦‚ä¸‹çš„
##### perceptron rule:

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/Screen%20Shot%202016-07-16%20at%2020.44.07.png?raw=true" height="109" width="434">

å¯ä»¥è®¤ä¸ºbiasæ˜¯å¯ä»¥è¡¡é‡æ„ŸçŸ¥å™¨è¾“å‡º1çš„éš¾åº¦çš„ï¼Œbiasè¶Šå¤§ï¼Œè¶Šå®¹æ˜“è¾“å‡º1ï¼Œåä¹‹å°±ä¼šæ¯”è¾ƒå›°éš¾ã€‚

å¦å¤–å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ„ŸçŸ¥å™¨å¯ä»¥æ„å»ºä»»ä½•é€»è¾‘é—¨ã€‚

é‚£ä¹ˆç°åœ¨çš„é—®é¢˜å°±æ˜¯ï¼Œèƒ½ä¸èƒ½è®¾è®¡å‡ºä¸€ç§å­¦ä¹ ç®—æ³•æ¥è‡ªåŠ¨è°ƒæ•´ä¸Šæ–‡æåˆ°çš„weightså’Œbiases.
ç­”æ¡ˆæ˜¯å¯ä»¥çš„ï¼Œå…¶ç¥å¥‡ä¹‹å¤„å°±åœ¨äºä¸ç”¨äººä¸ºåœ°å»è®¾è®¡é€»è¾‘é—¨ï¼Œç¥ç»ç½‘ç»œå¯ä»¥è‡ªå·±å­¦ä¹ å»è§£å†³é—®é¢˜ã€‚ä¸‹é¢å°±è¯´ä¸€ä¸‹æ˜¯å¦‚ä½•åšåˆ°çš„ã€‚

*****

### 2. Sigmoid neurons æ¿€åŠ±ç¥ç»å…ƒ
è¦è§£å†³å­¦ä¹ é—®é¢˜ï¼Œå…ˆå¼•å…¥sigmoid neuronsæ¿€åŠ±ç¥ç»å…ƒã€‚
æ¯”æ–¹è¯´æˆ‘ä»¬ç°åœ¨æœ‰å¼ æ„ŸçŸ¥å™¨æ„æˆçš„ç½‘ç»œï¼Œè¾“å…¥å°±æ˜¯ä¸€ä¸ªä¸ªæ‰‹å†™æ•°å­—å›¾ç‰‡çš„åƒç´ ã€‚æ€ä¹ˆé€šè¿‡è¿™ä¸ªç½‘ç»œæ¥å­¦ä¹ weightså’Œbiasesæ¥åšåˆ°æœ€åå¯ä»¥æ­£ç¡®åˆ†ç±»å‘¢ï¼Ÿ

é¦–å…ˆï¼Œå¯ä»¥ç†è§£çš„æ˜¯ï¼Œweightsæˆ–è€…biasesçš„ä¸€ç‚¹æ”¹å˜ï¼Œæ˜¯å¯ä»¥å¯¹æœ€åç»“æœäº§ç”Ÿä¸€ç‚¹å½±å“çš„ã€‚æ‰€ä»¥ï¼Œå‡è®¾ç½‘ç»œæŠŠ9è¯¯åˆ¤ä¸º8, é‚£è¦åšçš„äº‹å…¶å®å°±æ˜¯å¼„æ˜ç™½æ€ä¹ˆå»è°ƒæ•´ä¸€ç‚¹weightså’Œbiasesæ¥è®©ç½‘ç»œä½œå‡ºçš„åˆ¤å®šæ›´æ¥è¿‘æ­£ç¡®ç­”æ¡ˆ9ï¼Œå°±è¿™æ ·ä¸€ç‚¹ç‚¹è°ƒæ•´weightså’Œbiasesä½¿è¾“å‡ºæ›´æ¥è¿‘9ã€‚

ä¸Šé—®æåˆ°çš„æ„ŸçŸ¥å™¨éƒ½æ˜¯é€šè¿‡ä¸€äº›å¾ˆç®€å•çš„è®¡ç®—æ¥åšå†³ç­–çš„ï¼Œå¦‚æœä¸€å¼ ç½‘ç»œéƒ½æ˜¯ç”±è¿™æ ·çš„æ„ŸçŸ¥å™¨æ„æˆçš„ï¼Œé‚£ä¹ˆä¼šå‡ºç°ä¸€ä¸ªä¸å¯æ§åˆ¶çš„æƒ…å†µï¼šä¸€ä¸ªæƒé‡å˜äº†ç‚¹ç‚¹å¯¼è‡´ä¸€ä¸ªæ„ŸçŸ¥å™¨çš„è¾“å‡ºå®Œå…¨å˜äº†ï¼Œæ¯”å¦‚0ç›´æ¥å˜æˆäº†1.ç„¶åè¿™ä¸ªæ„ŸçŸ¥å™¨åˆå»å½±å“åˆ«çš„æ„ŸçŸ¥å™¨ï¼Œä¸€å‘ä¸å¯æ”¶æ‹¾ã€‚è¿™æ ·å°±å¾ˆéš¾åšåˆ°ç†æƒ³çŠ¶æ€ä¸­çš„ï¼Œæ…¢æ…¢æ”¹å˜weightså’Œbiasesæ¥è®©è¾“å‡ºæ›´å¥½ã€‚

æ‰€ä»¥æˆ‘ä»¬è¦å¼•å…¥ä¸€ä¸ªæ›´æ™ºèƒ½çš„äººå·¥ç¥ç»å…ƒâ€”â€”â€”â€”æ¿€åŠ±ç¥ç»å…ƒã€‚æ¿€åŠ±ç¥ç»å…ƒå’Œæ„ŸçŸ¥å™¨å¾ˆåƒï¼Œä½†æ˜¯å®ƒå¯ä»¥åšåˆ°weightsæˆ–biasesçš„ä¸€ç‚¹å˜åŒ–åªä¼šè®©outputä¹Ÿåªå—ä¸€ç‚¹å½±å“ã€‚è¿™æ ·å°±å…è®¸äº†æ¿€åŠ±ç¥ç»å…ƒæ„æˆçš„ç½‘ç»œå¯ä»¥è‡ªä¸»å­¦ä¹ ã€‚

é‚£ä¹ˆæ¿€åŠ±ç¥ç»å…ƒå’Œæ„ŸçŸ¥å™¨çš„ä¸åŒä¹‹å¤„å°±åœ¨äºï¼Œå®ƒçš„è¾“å‡ºä¸æ˜¯ç®€å•çš„0/1ï¼Œè€Œæ˜¯0ï¼1é—´çš„ä»»æ„æ•°å€¼ï¼Œæ¯”å¦‚ï¼Œ0.816....ä¹Ÿå°±æ˜¯Ïƒ(w â‹… x + b)
è¿™é‡Œçš„æ¿€åŠ±å‡½æ•°å®šä¹‰ä¸ºï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/s-function.png?raw=true" height="73" width="149">

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/s-full-function.png?raw=true" height="78" width="229">

ä¸ºä»€ä¹ˆè¦é‡‡å–è¿™ä¸ªå‡½æ•°ï¼Ÿå¯ä»¥çœ‹ä¸€ä¸‹å®ƒçš„å›¾åƒï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/sigmoid-function.png?raw=true" height="285" width="455" >

å¯ä»¥è®¤ä¸ºï¼Œè¿™ä¸ªæ¿€åŠ±å‡½æ•°ä½¿step functionå¹³æ»‘äº†ã€‚å¹³æ»‘ä¹Ÿå°±æ˜¯è¿™ä¸ªå‡½æ•°çš„å…³é”®æ‰€åœ¨ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥ä¿è¯weightså’Œbiasesçš„ä¸€ç‚¹å˜åŒ–ä½¿outputåªå—ä¸€ç‚¹å½±å“ã€‚
é€šè¿‡æ±‚åå¯¼ï¼Œå¯ä»¥å‘ç°ï¼ŒÎ”outputå’ŒÎ”w,Î”b å‘ˆçº¿æ€§çš„å˜åŒ–å…³ç³»ã€‚

*****

### 3. the architecture of neural network ç¥ç»ç½‘ç»œçš„ç»“æ„
æœ‰äº†sigmoid neuronsåï¼Œå†çœ‹å¦‚ä½•ç»„å»ºå®ƒä»¬ã€‚
ç¥ç»ç½‘ç»œä¸€èˆ¬ç”±è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚æ„æˆã€‚å…¶ä¸­éšè—å±‚å¯ä»¥æœ‰å¤šå±‚ã€‚
è¾“å…¥å±‚å’Œè¾“å‡ºå±‚çš„æ„é€ å¾€å¾€ç®€å•ç²—æš´ï¼Œå…³é”®ä¹‹å¤„è¿˜æ˜¯åœ¨äºéšè—å±‚çš„æ„é€ ã€‚ä¹‹åä¼šæåˆ°å¦‚ä½•å¯å‘å¼è®¾è®¡éšè—å±‚ï¼Œæ¥æƒè¡¡å¥½å±‚æ•°å’Œè®­ç»ƒæ—¶é—´çš„é—®é¢˜ã€‚

ä¹‹å‰æåˆ°çš„å‰ä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥è¿™ç§ç½‘ç»œï¼Œæˆ‘ä»¬ç§°å…¶ä¸º__feedforward neural networksï¼Œå‰å‘ç¥ç»ç½‘ç»œ__ï¼Œä¹Ÿå°±æ˜¯ä¸€æ°”å‘µæˆå‘å‰ä¼ é€’ï¼Œä¸å›å¤´ä¸å¾ªç¯çš„ã€‚å½“ç„¶ï¼Œä¹Ÿå­˜åœ¨æœ‰feedback loopçš„ç¥ç»ç½‘ç»œï¼Œé‚£ä¹Ÿå°±æ˜¯recurrent neural networksã€‚RNNä¸­ç¥ç»å…ƒçš„æ—¶æ•ˆæ€§ä½¿å…¶å¾ªç¯æˆä¸ºå¯èƒ½ã€‚RNNç›¸å¯¹æ¥è¯´æ²¡æœ‰å‰å‘ä¼ é€’é‚£ä¹ˆæœ‰åŠ›ï¼Œä½†æ˜¯å®ƒçš„æ—¶æ•ˆæ€§æ›´åƒäººç±»å¤§è„‘çš„è¿ä½œæ–¹å¼ï¼Œæ‰€ä»¥æ—¶å¸¸å¯ä»¥æ‹¿æ¥è§£å†³å‰å‘ç½‘ç»œä¸èƒ½è§£å†³çš„é—®é¢˜ã€‚

*****

### 4. a simple network to classify handwritten digits
ç”¨åˆ°çš„ç½‘ç»œå¦‚å›¾:

![nn](https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/3-layer-nn-recdigits.png?raw=true)

ç¬¬ä¸€å±‚æ˜¯ __è¾“å…¥å±‚__ ã€‚è¾“å…¥æ˜¯28*28=784ä¸ªåƒç´ (è¾“å…¥ç¥ç»å…ƒ), å¹¶ä¸”è¿™äº›æ˜¯é»‘ç™½å›¾ç‰‡ï¼Œ0ä»£è¡¨ç™½è‰²ï¼Œ1ä»£è¡¨é»‘è‰²ï¼Œ0-1é—´çš„æ•°å€¼è¡¡é‡ç°åº¦ã€‚

ç¬¬äºŒå±‚æ˜¯ __éšè—å±‚__ ã€‚é€šè¿‡è®­ç»ƒåå‘ç°è‡³å°‘éœ€è¦15ä¸ªç¥ç»å…ƒã€‚

ç¬¬ä¸‰æ¬¡æ˜¯ __è¾“å‡ºå±‚__ ã€‚æœ‰10ä¸ªç¥ç»å…ƒ,ä»£è¡¨0-9, å“ªä¸ªæœ€åçš„activation valueæœ€å¤§ï¼Œé‚£ä¹ˆå°±çŒœæµ‹è¾“å…¥çš„æ˜¯å“ªä¸ªæ•°å­—ã€‚

ä¸ªäººç†è§£ï¼Œéšè—å±‚é‡Œçš„ä¸€ä¸ªä¸ªç¥ç»å…ƒåˆ†åˆ«ä»£è¡¨äº†è¾“å…¥çš„ä¸åŒéƒ¨åˆ†(ç‰¹å¾)ï¼Œç„¶åç”±è¾“å‡ºå±‚çš„ç¥ç»å…ƒæ¥å¯¹æ‰€æœ‰ç‰¹å¾åŠ æƒï¼Œçœ‹åˆ°åº•æ˜¯å“ªä¸ªæ•°å­—ã€‚


*****

### 5. Learning with gradient descent
äºæ˜¯è¿˜æ˜¯æ²¡æœ‰è¯´åˆ°æ€ä¹ˆæ ·å»è®­ç»ƒã€‚è¿™é‡Œä¼šæåˆ°ç”¨gradient descentæ¢¯åº¦ä¸‹é™æ¥è¿›è¡Œå­¦ä¹ ã€‚

è¾“å…¥xï¼Œå°±æ˜¯ä¸€ä¸ª784ç»´çš„åˆ—å‘é‡ï¼›è¾“å‡ºy,å°±æ˜¯ä¸€ä¸ª10ç»´çš„åˆ—å‘é‡ã€‚
æ‰€ä»¥è®­ç»ƒçš„ç›®çš„å°±æ˜¯ è®©å¾—åˆ°çš„yå’Œå®é™…ä¸Šçš„ç»“æœè¯¯å·®æœ€å°ã€‚

é‚£ä¹ˆå…ˆå®šä¹‰ä¸€ä¸‹ç”¨æ¥è¡¡é‡è¯¯å·®çš„ä»£ä»·å‡½æ•°(cost or loss or objective function. )ã€‚
å¦‚ä¸‹å›¾ï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/cost-function.png?raw=true" height="71" width="279" />

å„ä¸ªç¬¦å·çš„è§£é‡Šï¼š
Here, w denotes the collection of all weights in the network, b all the biases, n is the total number of training inputs, a is the vector of outputs from the network when x is input, and the sum is over all training inputs x. y(x) is the desired output.	e.g.y(x)=(0,0,0,0,0,0,1,0,0,0)T

Cè¢«ç§°ä½œ __quadratic cost function__ ï¼Œ ä¹Ÿè¢«ç§°ä¸ºmean squared error(MSE)

è®­ç»ƒç›®æ ‡å¾ˆæ˜ç¡®ï¼ŒCè¶Šæ¥è¿‘0ä»£è¡¨åˆ¤æ–­è¶Šå‡†ç¡®ã€‚è¿™ä¸ª __minimize C__ çš„è¿‡ç¨‹å¯ä»¥ç”±æ¢¯åº¦ä¸‹é™æ³•æ¥å®ç°ã€‚

ï¼ˆè¿™é‡Œä½œè€…åˆæåˆ°äº†ï¼Œä¸ºä»€ä¹ˆä¸æœ€å¤§åŒ–åˆ¤æ–­å¯¹çš„ä¸ªæ•°æ¥è°ƒæ•´ç½‘ç»œã€‚å› ä¸ºä¸€ç‚¹ç‚¹çš„æƒé‡æˆ–è€…åå·®çš„æ”¹å˜å¾€å¾€ä¸ä¼šå¯¹ç»“æœäº§ç”Ÿè´¨çš„æ”¹å˜ã€‚æ­£ç¡®çš„ä¸ªæ•°ä¸æ˜¯å…³äºæƒé‡å’Œåå·®çš„smooth function!åˆæ˜¯smoothçš„é—®é¢˜ğŸ’è“è€Œquadratic functionæ˜¯å¹³æ»‘çš„ï¼Œç”¨å®ƒæ¥è¡¡é‡å°±ä¸ä¼šæœ‰é—®é¢˜ã€‚æœ€åå†ç”¨æ­£ç¡®çš„ä¸ªæ•°æ¥æ£€æŸ¥åˆ†ç±»çš„å‡†ç¡®æ€§è€Œä¸æ˜¯ç”¨è¿™ä¸ªå‡†ç¡®æ€§å»è®­ç»ƒç½‘ç»œï¼‰

è¿™é‡Œçš„quadratic functionåé¢è¿˜ä¼šå†æ”¹ï¼Œè¿™é‡Œå…ˆç†è§£å®ƒçš„æ„ä¹‰å°±è¡Œã€‚

é¦–å…ˆï¼Œå…ˆå¿˜æ‰ç»ç½‘ç»œçš„ç§ç§ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå…ˆçœ‹å¦‚ä½•ç”¨æ¢¯åº¦ä¸‹é™æ³•å»æœ€å°åŒ–ä¸€ä¸ªæœ‰å¾ˆå¤šå˜é‡çš„functionã€‚ç„¶åå†å›åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œè¿ç”¨æ¢¯åº¦ä¸‹é™å»è§£å†³é—®é¢˜ã€‚
å·¨å¤§çš„ç¥ç»ç½‘ç»œä¼šå‡ºç°çº¬åº¦å™©æ¢¦ï¼Œå•å•é è®¡ç®—æ¥æ±‚æœ€å°å€¼æ˜¯è¡Œä¸é€šçš„ã€‚

æ¢¯åº¦ä¸‹é™æ³•ä¸»è¦ç‰µæ‰¯åˆ°æ¢¯åº¦(æ–¹å‘)å’Œæ­¥é•¿ã€‚æ¢¯åº¦âˆ‡Cå°±æ˜¯å¯¹Cæ±‚åå¯¼ï¼Œå¯ä»¥æŠŠå­¦ä¹ ç‡è®¤ä¸ºæ˜¯æ­¥é•¿ã€‚
å¦å¤–å°±æ˜¯æ¢¯åº¦ä¸‹é™æ³•æ€»æ˜¯é€‰æ‹©å½“å‰ä¸‹é™æœ€å¿«çš„é‚£æ¡è·¯èµ°ï¼Œæ‰€ä»¥æ±‚å¾—çš„æ˜¯å±€éƒ¨æœ€ä¼˜è§£ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œæ¢¯åº¦ä¸‹é™åœ¨å­¦ä¹ ä¸­è¡¨ç°å·²ç»ä»¤äººæ»¡æ„äº†ã€‚

ç»“åˆåˆ°ç¥ç»ç½‘ç»œã€‚

æ‰¹é‡æ¢¯åº¦ä¸‹é™çš„è¿­ä»£å¦‚ä¸‹:

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/der-wb-mgd.png?raw=true" height="123" width="254" />

å› ä¸ºè®¡ç®—Cæ—¶ï¼Œæ˜¯å¯¹æ¯ä¸ªè¾“å…¥åŠ å’Œå¹³å‡ï¼Œâˆ‡Cä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæ‰€ä»¥å½“è¾“å…¥æ ·æœ¬å¾ˆå¤šçš„æ—¶å€™ï¼Œä¼ ç»Ÿbatch gradient descentè®¡ç®—é‡ä¼šéå¸¸ä¹‹å¤§ï¼Œå­¦ä¹ é€Ÿåº¦å°±ä¼šå¾ˆæ…¢ã€‚

ä¸‹é¢ä»‹ç»__mini-batch gradient descent__æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡ç®—æ¢¯åº¦çš„æ—¶å€™åªæ˜¯ä»æ ·æœ¬é‡ŒæŠ½å–éƒ¨åˆ†xå‡ºæ¥åŠ å’Œå¹³å‡ï¼Œå°†å…¶è§†ä½œå…¨éƒ¨è¾“å…¥çš„è¿‘ä¼¼ï¼Œè¿™æ ·ä¹Ÿå¯ä»¥å–å¾—ä¸é”™çš„æ•ˆæœã€‚
è¿™äº›æœ‰å¹¸è¢«é€‰ä¸­çš„mä¸ªæ ·æœ¬çš„é›†åˆè¢«ç§°ä¸º__mini-batch__. è¿™ä¸ªæ ·æœ¬é›†åˆè¦å¤§åˆ°å¯ä»¥ä½œä¸ºå…¨éƒ¨çš„è¿‘ä¼¼æ‰å¯ä»¥ï¼Œé€šå¸¸æ˜¯10ã€‚

å‡è®¾æ‰€æœ‰æ ·æœ¬è¢«åˆ†ä¸ºäº†n/mä¸ªmini-batch, é‚£ä¹ˆæ¯è½®è¿­ä»£éƒ½é€‰ä¸€ä¸ªmini-batchå‡ºæ¥ï¼Œç›´åˆ°æ‰€æœ‰çš„éƒ½è¢«é€‰è¿‡ï¼Œè¿™æ—¶ç§°ä¸ºå®Œæˆäº†ä¸€ä¸ª__epoch of training__ã€‚ç„¶åå¯ä»¥ç»§ç»­å¼€å§‹ä¸€ä¸ªnew training epoch. 

å½“mini-batché‡Œçš„æ ·æœ¬å€¼ï¼1çš„æ—¶å€™ ç§°ä¸º__stochastic gradient descentéšæœºæ¢¯åº¦ä¸‹é™__ æ³¨æ„è¿™ä¸ªæ—¶å€™æ¯æ¬¡è¿­ä»£å¹¶ä¸æ˜¯æœç€æœ€ä¼˜çš„æ–¹å‘è¿›è¡Œçš„ åªæ˜¯æ¯æ¬¡ç®—æ¢¯åº¦éƒ½éšæœºæ‹¿ä¸ªæ ·æœ¬æ¥ç®— è¿™æ ·çš„è¯éœ€è¦è¿­ä»£æ¬¡æ•°è¾ƒå¤šï¼Œè€Œæœ‰æ—¶å€™å¯ä»¥é¿å…æ‰¹é‡ä¸‹é™æ—¶å€™é‡åˆ°çš„å±€éƒ¨æœ€ä¼˜è§£é—®é¢˜ã€‚è¿˜æœ‰ä¸€ä¸ª__Online GD__å’ŒSGDçš„åŒºåˆ«å°±æ˜¯æ‰€æœ‰æ•°æ®åªç”¨ä¸€æ¬¡å°±ä¸¢å¼ƒã€‚ï¼ˆè¿™è¾¹Michael Nielsenå¥½åƒæ··æ·†äº†mini-batch GDï¼ŒSGDï¼ŒOnline GDï¼Ÿæˆ–è€…è¯´ä»–å¯¹è¿™äº›æ–¹æ³•çš„å‘½åå’Œæ™®é€šæ–¹å¼ä¸ä¸€æ ·ï¼‰

å¯¹äº__ä¸‰è€…çš„åŒºåˆ«__æˆ‘çš„ç†è§£æ˜¯ï¼š
æŠŠæ¢¯åº¦ä¸‹é™çœ‹ä½œæ˜¯ä¸‹å±±é—®é¢˜ï¼Œæ ·æœ¬ä»¬å…±åŒå†³å®šå¦‚ä½•ä¸‹å±±(é€‰å–æ¢¯åº¦)ã€‚

__æ‰¹é‡æ¢¯åº¦ä¸‹é™__ å°±æ˜¯æ¯ä¸ªæ ·æœ¬éƒ½é€‰ä¸ªæ–¹å‘ï¼Œç®—ä¸€ä¸‹å¤§å®¶çš„å¹³å‡å€¼ï¼Œç„¶åå¤§å®¶é›†ä½“è¿ˆå‡ºä¸€æ­¥ã€‚æ¯ä¸€æ­¥éƒ½æ˜¯å¦‚æ­¤ã€‚

__éšæœºæ¢¯åº¦ä¸‹é™__ å°±æ˜¯ç»™æ ·æœ¬ä»¬æ’ä¸€ä¸ªåšå†³å®šçš„æ¬¡åºï¼Œæ¯èµ°ä¸€æ­¥éƒ½æŒ‰ç…§ä¹‹å‰è®¢å¥½çš„æ¬¡åºè®©é‚£ä¸ªæ ·æœ¬æ¥åšå†³å®šï¼Œå¤§å®¶è·Ÿç€å®ƒçš„æ–¹å‘èµ°ï¼Œç„¶åä¸‹ä¸€æ­¥å†æŒ‰ç…§çº¦å®šå¥½çš„æ¬¡åºæ¢ä¸ªæ ·æœ¬æ¥å†³å®šæ€ä¹ˆèµ°ã€‚

__mini-batch GD__ å°±æ˜¯ç»™æ ·æœ¬ä»¬å…ˆè¿›è¡Œåˆ†ç»„ï¼Œå…ˆæ’å¥½å°ç»„é¡ºåºï¼Œç»„å†…æŠ•ç¥¨å®Œèµ°å“ªä¸ªæ–¹å‘ï¼Œç„¶åå¤§å®¶è·Ÿç€ä»–ä»¬ç»„èµ°ã€‚è¿™æ ·ä¸€è½®ç»“æŸåï¼Œå†é‡å¤ä»¥ä¸Šæ­¥éª¤ã€‚é‡å¤çš„æ¬¡æ•°ä¹Ÿå°±æ˜¯ä¸€å¼€å§‹çº¦å®šå¥½çš„epchoä¸ªæ•°ã€‚

ç°åœ¨å°†mini-batchè¿ç”¨åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œé‚£ä¹ˆè¿­ä»£æ–¹æ³•å¦‚å›¾ï¼š

<img src="https://github.com/Bugix-ZY/Bugix-ZY.github.io/blob/master/images/postImages/netural-network-1/mgd.png?raw=true" height="151" width="305" />


*****

### 6.ä¾‹å­
çœ‹ä¸€ä¸ªMNISTçš„è®­ç»ƒä¾‹å­ã€‚50000åštraining set, 10000åšvalidation set(è¿™é‡Œæš‚ä¸”æ²¡ç”¨)ã€‚

__ä¸€äº›å…³é”®çš„åœ°æ–¹__

1 åˆå§‹åŒ–ä¸€ä¸ªnet, weightså’Œbiaseséƒ½éšæœºé«˜æ–¯åˆ†å¸ƒï¼Œä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œéƒ½å‘é‡åŒ–ã€‚

    class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]

    å…³äºnumpyï¼š
    >>> sizes
    [2, 3, 1]
    >>> sizes[1:]
    [3, 1]
    >>> sizes[:-1]
    [2, 3]
    >>> numpy.random.randn(3,1)
    array([[ 0.28596078],
            [ 1.36472491],
            [ 0.76747929]])
    >>> numpy.random.randn(2,1)
    array([[ 0.59676856],
            [-0.74950995]])

è¿™æ ·å°±ç†è§£biaseså’Œweightsçš„ç»´åº¦äº†ğŸ¶


2 å®šä¹‰å‰é¦ˆæ–¹æ³•ï¼Œä¸€ä¸ªå¾ªç¯å°±æå®šå•¦ã€‚

    def feedforward(self, a):
        """Return the output of the network if "a" is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a

3 ç„¶åå°±æ˜¯å¦‚ä½•è°ƒæ•´weightså’Œbiasesçš„é—®é¢˜äº†

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The "training_data" is a list of tuples
        "(x, y)" representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If "test_data" is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

å‚æ•°è§£é‡Šï¼šepochsæ˜¯epochçš„æ•°é‡ï¼Œmini_batch_sizeæ˜¯batchçš„é‡Œæ ·æœ¬çš„æ•°é‡ï¼Œetaæ˜¯å­¦ä¹ é€Ÿç‡ã€‚

mini_batchesæ˜¯kä¸ªmini_batchçš„é›†åˆ ä¹Ÿå°±æ˜¯æ‰“ä¹±äº†çš„training_dataã€‚

å¯ä»¥çœ‹åˆ°æ¯ä¸ªepchoä¸­ï¼Œæ‰“ä¹±æ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œåˆ†æˆkä¸ªbatchï¼Œç”¨æ¯ä¸ªbatchè¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚

ä¸‹é¢è§£é‡Šupdate_mini_batch(mini_batch, eta)è¿™ä¸ªå‡½æ•°ï¼

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]
self.backprop(x, y)ç‰µæ‰¯åˆ°äº†backpropagation algorithmï¼Œbpæ˜¯ä¸€ä¸ªå¿«é€Ÿè®¡ç®—cost functionçš„æ¢¯åº¦çš„æ–¹æ³•ã€‚

bpçš„ç†è®ºå’Œä»£ç å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­å†è¯´ğŸ’

ç°åœ¨å°±è®¤ä¸ºè¿™ä¸ªå‡½æ•°å¯ä»¥è¿”å›æ­£ç¡®çš„æ¢¯åº¦

æœ€å
Networks with this kind of many-layer structure - two or more hidden layers - are called deep neural networks.people now routinely train networks with 5 to 10 hidden layers. And, it turns out that these perform far better on many problems than shallow neural networks, i.e., networks with just a single hidden layer. The reason, of course, is the ability of deep nets to build up a complex hierarchy of concepts. 

