---
layout: post
title: "Deep Learning Specialization - Neural Networks and Deep Learning"
date: 2017-11-11
excerpt: "Deep Learning Specialization — Andrew Ng 系列课程"
tags: [deep learning]
comments: true
---

**Reference**:

- [Coursera - Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning/home/info)

---

毕业设计会是深度学习相关，因此想系统学习深度学习领域的知识。正好 Andrew Ng 现在开了 Deep Learning Specialization 系列课程，一共有五门课程：

1. Neural Networks and Deep Learning
2. Improving Deep Neural Networks：Hyperparameter tuning, Regularization and Optimization
3. Structuring your Machine Leaning Project
4. Convolutional Neural Networks
5. Natural Language Processing: Building equence models

这篇博文针对第一门课 —— **Neural Networks and Deep Learning**



## Week 1  Introduction to Deep Learning

### What is Neural Network?

深度学习一般就是说训练神经网络。

![house-price-prediction](../../images/postImages/nn-and-dl/house-price-prediction.png)

* a **neuron** can be 
  - ReLU  function
* 隐藏层其实完全由网络自己来弄清楚（决定每个隐藏神经元代表什么），我们只需要给网络大量的训练数据（即输入和输出）
* 神经网络在监督式学习中，可以算是最强有力的算法


 



### Supervised Learning with Neural Networks

* Structured Data 🆚 Unstructured Data

  * structured data：每个属性都有清晰的定义
  * unstructured data：比如图片、文本中的像素、单词

  现在深度学习可以胜任 unstructured data 了


  ​

### Why is Deep Learning taking off?

深度学习的概念在三十多年前就有了，但是知道最近几年才发展得如此迅猛，其原因在于：

![scale](../../images/postImages/nn-and-dl/scale.png)

- **scale**：

  - Data：一方面说的是需要巨大的训练数据
  - Computation：另一方面说的是神经网络中大量的神经元、连接、参数（训练时间）
  - Algorithms：这几年冒出很多新算法（more efficient，e.g. sigmoid(有些地方梯度接近于 0 导致训练很慢) -> ReLU）

- small training sets:

  当数据集比较小的时候，很难说传统算法和神经网络哪个更好。算法的表现*取决于我们手动选择特征的技巧*。

  但是当数据量真的非常大的时候（图像右侧区域），NN 会碾压传统算法。（多大算大？？


## Week 2  Neural Networks Basics 

本周将学到：神经网络如何可以一口气处理整个训练集（m 个 training sample 在神经网络中不需要 for 循环来显示的一个个处理）；前向传播和后向传播；

### Binary Classification

* 计算机如何表示一个彩色的图像：RGB 三个矩阵 -> 再展开成一个列向量
* 二元分类 Notation：
  * $n_x$ 或者 $n$ : feature 的数量
  * $(x, y)$： $x \in R^{n_x}, y \in \{0, 1\}$
  * $m\  个 \ training \ samples $ -> $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}),…, (x^{(m)}, y^{(m)})\}$
  * $X =  \left [ \begin{matrix}   | & | & ... & | \\   x^{(1)} & x^{(2)} & … & x^{(m)} \\    | & | & ... & |  \end{matrix}  \right]  $ 表示所有训练集, $X \in R^{n_x * m}$
  * $Y = [y^{(1)}, y^{(2)}, … , y^{(m)}] \in R^{1 * m}$

  像有的时候 大X 是横着堆叠的，第 i 行就是$x_i^T$，那种情况下 y是列向量，矩阵运算 y = XB；


###Logistic Regression

举例：是否是猫的二元分类问题

**问题定义**

$ Given \ x, want \  \hat y = P \{ y = 1 \ | \ x\}, where \ 0 ≤ \hat y ≤ 1. \ \hat y \ 就是对真实 \ y=1 \ 的估计$

**参数**

![parameters](../../images/postImages/nn-and-dl/parameters.png)

  

因为想要 $\hat y $ 在 0-1之间，单用 linear 方程就不合理了（结果可以为负，可以远大1），于是用 sigmoid 方程来拟合。  

需要注意的地方：

- if $z$ is very large, $\sigma (z) = 1$
- if $z$ is very small, $\sigma (z) = 0 $
- if $z = 0, \sigma (z) = 0.5$    

我们的任务就是学习参数 $w \  和 \ b$，以此得到更好的 $\hat y$   

还有一种符号约定方式是，添加了$ x_0 = 1$, 直接用 $\theta_0$ 来直接代替 $b$。 实际上在实现神经网络当中，将 $w$ 和 $b$ 作为互相独立的参数会更简单。  

**损失函数**

Loss Function 衡量 单个样本：$  L(\hat y, y) = -(ylog\hat y + (1-y)log(1-\hat y))$  

Cost Function 衡量 全体样本：$J(w, b) = 1/m \sum L(\hat y^{(i)}, y^{(i)}  ) = -1/m \sum [y^{(i)}log(\hat y^{(i)}) + (1 - y^{(i)})log(1-\hat y^{(i)})]$



**梯度下降法**

用梯度下降法来学习参数 w, b —> minimize J(w,b) <— J 是凸函数 有全局最优解

Repeat {

​	$w := w - \alpha \frac{\partial J(w,b)}{\partial w}$

​	$b := b - \alpha \frac{\partial J(w,b)}{\partial b}$

} until convergence

$\alpha：学习率，步长$



### 导数

回顾导数

$\frac {df(a)}{da}$ 的意思基本就是，如果 a 变了一点点， f(a) 相应变化了多少倍。类似斜率，三角的高除以宽  

比如说 $f(a) = 3a$ ，a 增加 0.0001，那么a 对 f(a) 的影响是： f(a) 随之增加 0.0003

1. 导数就是斜率
2. 一个函数上的不同点可能有不同的斜率 e.g.  $ y = x^2$





### 计算图和其导数计算

* 一个函数可以按照步骤表达成一个流程图，计算这个函数只要跟着图从左到右就可以了。也就是神经网络中所谓的**前向传播**。
* 计算一个函数的导数，一种简单的做法是从右到左来计算。也就是所谓的**反向传播**。

#### 简单的例子

一个简单的例子，与 a,b,c 三个变量相关的函数 $J(a, b, c) = 3 (a + bc)$，计算这个函数实际上有三个步骤： 

1. 先算 bc  
2. 再算 a + bc  
3. 最后乘以 3  

接着设置一些中间变量：

$u = bc  $

$v = a + u$

$J = 3v $

然后就能画成如下的流程图：

这样的计算图适用于计算一些特殊的输出，比如我们想要优化的逻辑回归中的成本函数

![computation-graph-1](../../images/postImages/nn-and-dl/computation-graph-1.png)



Instead of $ 3 * (5 + 3 * 2) = 33$，我们用图的方式来计算出了最终答案 33。同样，我们也可以利用计算图一步一步来求 a,b,c 的偏导。  

蓝色箭头是计算 J 的方向，红色箭头就是接下来要说的求导的方向。  

第一个反向传播-> $\frac{dJ}{dv} = 3$，对于任何 v 的增量， J 都会有三倍增量

那么 $\frac{dJ}{da}$是多少呢？如果增加了 a 的数值，那对 J 的数值有什么影响？

（a 的改变会在图中传播，影响到最终结果 J）

如果改变了 a, 那么就改变了 v, 那么就改变了 J.  这就是 **链式法则** 了 

$\frac{dJ}{da}$ = $\frac{dJ}{dv}·\frac{dv}{da}$

所以说，第一次反向传播得到的 dv， 能够帮助计算下一步的反向传播 da  

同理，我们可以通过 dv 算出 du，再通过 du 算出 db 和 dc



#### Logistic 回归的例子

之前有提到，逻辑回归可用梯度下降法寻找到最优解。

这里会详细说怎样利用计算图来计算偏导数完成梯度下降，这对逻辑回归来说有些overkill了，但是能帮助我们提前认识神经网络。

**假设只有一个样本，二维特征：**

$z = w^Tx + b$

$\hat y = a = \sigma(z)$

$L(a, y) = -(ylog(a)+ (1-y)log(1-a))$

计算步骤是先算 z, 再算 y hat, 再算 L。我们的目标是调整 wb 去 minimize L，也就是计算损失函数L对w和b的偏导。

![computation-graph-logistic](../../images/postImages/nn-and-dl/computation-graph-logistic.png)

和之前的步骤一样，我们先算 $\frac{dL}{da} = -\frac{y}{a} + \frac{1-y}{1-a}$

第二个反向传播，$\frac{dL}{dz} = \frac{dL}{da}·\frac{da}{dz}$, 其中 $\frac{da}{dz} = a(1 - a)$,所以$\frac{dL}{dz} = a-y$

同理再一个反向传播，可以求得 dw1, dw2 和 db

dw1 = (a - y) * x1  

dw2 = (a - y) * x2  

db = a - y  

**假设有 m 个样本，二维特征：**

在有 m 个样本的情况下，成本函数是m个样本的损失函数的均值。导数从 v也是同理，是各项损失函数求导的均值。

![lr-on-m](../../images/postImages/nn-and-dl/lr-on-m.png)

上图中的计算步骤只是做了一次梯度下降，如果要多次，就要重复所有步骤多次。

这种梯度下降方法有一个很大的缺点。就是里面有**显式**的 2个 for 循环。一个是对所有样本的循环 1 to m，一个是对所有特征的循环 1 to n（这里只有两个特征）。在深度学习中，数据集非常大，需要通过**向量化**来提高效率。



### 向量化 Vectorization

尽量避免 For 循环。For 循环真的很慢。

#### Logistic 回归例子

**前向传播**

`Z = np.dot(w.T, X)+ b`  

`A = σ(Z)  `  

其中 w 为列向量， X 是 n * m 的矩阵，b 可以是实数，以为 python 和 numpy 中存在一个 “broadcasting” 的技术可以自动将 b 转化成行向量。  

**后向传播**

```
dZ = A - Y
dW = 1/m * X * dZ.T
db = 1/m * np.sum(dZ)
```

#### Python 中的广播

e.g. 求矩阵中 各列的和

`cal = A.sum(axis=0)	// axis=0 表示列，纵向`

#### NumPy 向量

要注意 a = np.random.randn(5) 得到的会是 *rank 1 array* ，既不是行向量也不是列向量。要 a = np.random.randn(5, 1) 得到的才是 5 * 1 的列向量

不确定的时候可以用assert() e.g. assert (a.shape == (5, 1))





##Week 3  浅层神经网络

### 神经网络概览

#### 3.7 为什么需要非线性激活函数

不然就只是特征的线性组合了

有一种情况下可以用 g(z) = z 这样的线性激活函数：输出层。当 $\hat y$ 是 real number



####3.8 激活函数的导数







#### 3.11 随机初始化 

初始化不可以都设置成0

w = np.random.randn((? * ?)) * 0.01// 不希望w太大，避免梯度下降速度过慢











