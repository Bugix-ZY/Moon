---
layout: post
title:  Natural Computing - Simulated Annealing
description:  Simulated Annealing & Particle Swarm Optimization
category: blog
---


# Lesson 2: Simulated Annealing
## 2a. Simulated Annealing

### What is Simulated Annealing
Simulated annealing (SA) is a ~global optimization~ algorithm inspired by the physical process of the thermal annealing of metals or glass
[annealing: cooling of material.  - If you heat a solid past melting point and then cool it, the structural properties of the solid depend on the rate of cooling. If the liquid is cooled slowly enough, large crystals will be formed. However, if the liquid is cooled quickly (quenched) the crystals will contain imperfections.]

 

**brief idea**
the physical process of thermal annealing = search for the global minimum of an energy landscape (find the global minimum of an objective function)
starting with a high temperature and slowly cooling down

 **detailed idea** (see related criterias in next section)

* Each point **s** of the search space = a state of some physical system
* The objective function **E(s)** = internal energy of the system in that state
* Iteratively random mutations **s'** of the current state **s** are generated, and accepted or rejected using the **Boltzmann-Gibbs distribution**

 

State  =  Feasible solution to the problem   

Energy = Value returned by eval() ​    

Equilibrium State = Local Optimum Ground State = Global Optimum ​      

Temperature = Control Parameter   

Annealing = Search by reducing T    

Boltzmann-Gibbs distribution = Probability of selecting a new point  	    

  


### Acceptance Criteria
The probability of finding the system in a particular energy state is given by the **Boltzmann-Gibbs distribution**
- The law of thermodynamics state that 	*P(∆E) = exp(-∆E / kT)*  // P(∆E) 出现能量差为dE的降温的概率
  Where k is a constant known as Boltzmann’s constant.
- Probability of finding the system in state *x*:
   *P (𝑥⃗) ∝ exp(−E(𝑥⃗) / kT)*
   Through the formula, we can see that if T increases, system becomes more active -> probability for each individual particle of being in a high energy  state increases, vice versa.
   The higher the value of Ti, the higher the likelihood that a poorer solution will be accepted. 
### How does it escape from the local optimum
* Simulated annealing chooses a random move from the neighborhood.	* If the move is better than its current position then simulated annealing will always take it. 
  * If the move is worse (i.e. lesser quality) then it will be accepted based on some probability. (which means ~occasional ‘wrong-way’, or uphill, moves are accepted with a nonzero probability~!)

### Pseudo Code
```
create initial solution s
Set initial temperature T
while not terminate do  
	repeat k times    
		s’ = perturb s    
		∆E = E(s’) - E(s)    
		if (∆E <= 0) then      
			s = s’    
		else if (rnd(0,1) < exp(-∆E/T)) then		
			s = s’ 
		end if  
	end repeat  
	decrease T
end do
```

* outer loop - temperature : ~controls the probability of uphill moves~
* **~At lower temperatures, the finer details of this area emerge and, with high probability, the algorithm will find a near-global optimum, or even the global optimum.~**

### Cooling Schedules
* Starting Temperature- Final Temperature: usually zero- Temperature Decrement : exponential- Iterations at each temperature 
  - fixed, or
  - dynamically change the number of iterations as the algorithm progresses. At lower temperatures it is important that a large number of iterations are done so that the local optimum can be fully explored. At higher temperatures, the number of iterations can be less.

### Pros & Cons
* Pros: 
  1. avoid getting stuck at local minima
  2. guarantees a convergence upon running sufficiently large (infinite) number of iterations
* Cons:
  1. Determining the “cooling schedule” is difficult (how do to decide what is a sufficient amount of iterations at each temperature)
  2. Determining the initial temperature is difficult.


### Summary
* Early in the run, gross features of the system appear. The algorithm finds a broad area in the search space where a global optimum should exist, following the large-scale behavior without regard to small local optima found on the way. At lower temperatures, the finer details of this area emerge and, with high probability, the algorithm will find a near-global optimum, or even the global optimum.一开始的时候要温度高是因为要有全局观，忽略在local optimum 允许更多的错误来接近 global optimum；
* large-scale -> finer-scale
* T 下降越慢，越可能找到全局最值。



## 2b. Particle Swarm Optimization 粒子群优化算法
### Backgrounds
![PSO](../../images/postImages/2017-10-29-sa/PSO.png)

### General Concept
* Each particle residing at a **position** in the search space
* **Fitness** of each particle = the quality of its position
* Particles fly over the search space with a certain **velocity***  Velocity (both direction and speed) of each particle is influenced by its own best position found so far and the best solution that was found so far by its **neighbors**
  **what is neighborhoods?**
  - The neighborhood of each particle is defines its communication structure (its social network)
    type 1: Geographical neighborhood topologies 
    type 2 (common): Communication network topologies
* Eventually the swarm will converge to optimal positions


### Algorithm
```
Randomly initialize particle positions and velocities 
	While not terminate
		For each particle i:		
        - Evaluate fitness yi at current position xi		 
        - If yi is better than pbesti then update pbesti and pi		 
        + If yi is better than gbesti then update gbesti and gi 
		// 这里是同步更新，如果是异步更新，那这句就要挪出这个 for 循环
		For each particle
		 - Update velocity vi and position xi using:
```
![code](../../images/postImages/2017-10-29-sa/code.png)

![formula](../../images/postImages/2017-10-29-sa/formula.png)else need to know ….**

* Synchronous versus Asynchronous
* Acceleration Coefficients  [some insights]
* [-vmax, vmax] 
  * 可以用 inertia weight - w 取代
  - FIPS 
  - bare bones PSO ，根据概率分布而不是 velocity 来迭代



适用于
- PSO is applicable for the optimization of hard multi-dimensional non-linear functions
  问题
- Theoretical Analysis is hard! because
  - Stochastic search algorithm	- Complex group dynamics	- Performance depends on the search landscape
    优点
- Regarding performance PSO is competitive to other known global optimization methods
