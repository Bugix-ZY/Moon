---
layout: post
title:  Natural Computing - Simulated Annealing
description:  Simulated Annealing & Particle Swarm Optimization
category: blog
---


# Lesson 2: Simulated Annealing
## 2a. Simulated Annealing

### What is Simulated Annealing
Simulated annealing (SA) is a ~global optimization~ algorithm inspired by the physical process of the thermal annealing of metals or glass
[annealing: cooling of material.  - If you heat a solid past melting point and then cool it, the structural properties of the solid depend on the rate of cooling. If the liquid is cooled slowly enough, large crystals will be formed. However, if the liquid is cooled quickly (quenched) the crystals will contain imperfections.]

 

**brief idea**
the physical process of thermal annealing = search for the global minimum of an energy landscape (find the global minimum of an objective function)
starting with a high temperature and slowly cooling down

 **detailed idea** (see related criterias in next section)

* Each point **s** of the search space = a state of some physical system
* The objective function **E(s)** = internal energy of the system in that state
* Iteratively random mutations **s'** of the current state **s** are generated, and accepted or rejected using the **Boltzmann-Gibbs distribution**

 

State  =  Feasible solution to the problem   

Energy = Value returned by eval()â€¨â€‹    

Equilibrium State = Local Optimumâ€¨Ground State = Global Optimumâ€¨â€‹      

Temperature = Control Parameterâ€¨  

Annealing = Search by reducing T    

Boltzmann-Gibbs distribution = Probability of selecting a new point  	    

  


### Acceptance Criteria
The probability of finding the system in a particular energy state is given by the **Boltzmann-Gibbs distribution**
- The law of thermodynamics state that 	*P(âˆ†E) = exp(-âˆ†E / kT)*  // P(âˆ†E) å‡ºç°èƒ½é‡å·®ä¸ºdEçš„é™æ¸©çš„æ¦‚ç‡
  Where k is a constant known as Boltzmannâ€™s constant.
- Probability of finding the system in state *x*:
   *P (ğ‘¥âƒ—) âˆ exp(âˆ’E(ğ‘¥âƒ—) / kT)*
   Through the formula, we can see that if T increases, system becomes more active -> probability for each individual particle of being in a high energy  state increases, vice versa.
   The higher the value of Ti, the higher the likelihood that a poorer solution will be accepted. 
### How does it escape from the local optimum
* Simulated annealing chooses a random move from the neighborhood.	* If the move is better than its current position then simulated annealing will always take it. 
  * If the move is worse (i.e. lesser quality) then it will be accepted based on some probability. (which means ~occasional â€˜wrong-wayâ€™, or uphill, moves are accepted with a nonzero probability~!)

### Pseudo Code
```
create initial solution s
Set initial temperature T
while not terminate do  
	repeat k times    
		sâ€™ = perturb s    
		âˆ†E = E(sâ€™) - E(s)    
		if (âˆ†E <= 0) then      
			s = sâ€™    
		else if (rnd(0,1) < exp(-âˆ†E/T)) then		
			s = sâ€™ 
		end if  
	end repeat  
	decrease T
end do
```

* outer loop - temperature : ~controls the probability of uphill moves~
* **~At lower temperatures, the finer details of this area emerge and, with high probability, the algorithm will find a near-global optimum, or even the global optimum.~**

### Cooling Schedules
* Starting Temperature- Final Temperature: usually zero- Temperature Decrement : exponential- Iterations at each temperature 
  - fixed, or
  - dynamically change the number of iterations as the algorithm progresses. At lower temperatures it is important that a large number of iterations are done so that the local optimum can be fully explored. At higher temperatures, the number of iterations can be less.

### Pros & Cons
* Pros: 
  1. avoid getting stuck at local minima
  2. guarantees a convergence upon running sufficiently large (infinite) number of iterations
* Cons:
  1. Determining the â€œcooling scheduleâ€ is difficult (how do to decide what is a sufficient amount of iterations at each temperature)
  2. Determining the initial temperature is difficult.


### Summary
* Early in the run, gross features of the system appear. The algorithm finds a broad area in the search space where a global optimum should exist, following the large-scale behavior without regard to small local optima found on the way. At lower temperatures, the finer details of this area emerge and, with high probability, the algorithm will find a near-global optimum, or even the global optimum.ä¸€å¼€å§‹çš„æ—¶å€™è¦æ¸©åº¦é«˜æ˜¯å› ä¸ºè¦æœ‰å…¨å±€è§‚ï¼Œå¿½ç•¥åœ¨local optimum å…è®¸æ›´å¤šçš„é”™è¯¯æ¥æ¥è¿‘ global optimumï¼›
* large-scale -> finer-scale
* T ä¸‹é™è¶Šæ…¢ï¼Œè¶Šå¯èƒ½æ‰¾åˆ°å…¨å±€æœ€å€¼ã€‚



## 2b. Particle Swarm Optimization ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•
### Backgrounds
![PSO](../../images/postImages/2017-10-29-sa/PSO.png)

### General Concept
* Each particle residing at a **position** in the search space
* **Fitness** of each particle = the quality of its position
* Particles fly over the search space with a certain **velocity***  Velocity (both direction and speed) of each particle is influenced by its own best position found so far and the best solution that was found so far by its **neighbors**
  **what is neighborhoods?**
  - The neighborhood of each particle is defines its communication structure (its social network)
    type 1: Geographical neighborhood topologies 
    type 2 (common): Communication network topologies
* Eventually the swarm will converge to optimal positions


### Algorithm
```
Randomly initialize particle positions and velocities 
	While not terminate
		For each particle i:		
        - Evaluate fitness yi at current position xi		 
        - If yi is better than pbesti then update pbesti and pi		 
        + If yi is better than gbesti then update gbesti and gi 
		// è¿™é‡Œæ˜¯åŒæ­¥æ›´æ–°ï¼Œå¦‚æœæ˜¯å¼‚æ­¥æ›´æ–°ï¼Œé‚£è¿™å¥å°±è¦æŒªå‡ºè¿™ä¸ª for å¾ªç¯
		For each particle
		 - Update velocity vi and position xi using:
```
![code](../../images/postImages/2017-10-29-sa/code.png)

![formula](../../images/postImages/2017-10-29-sa/formula.png)else need to know â€¦.**

* Synchronous versus Asynchronous
* Acceleration Coefficients  [some insights]
* [-vmax, vmax] 
  * å¯ä»¥ç”¨ inertia weight - w å–ä»£
  - FIPS 
  - bare bones PSO ï¼Œæ ¹æ®æ¦‚ç‡åˆ†å¸ƒè€Œä¸æ˜¯ velocity æ¥è¿­ä»£



é€‚ç”¨äº
- PSO is applicable for the optimization of hard multi-dimensional non-linear functions
  é—®é¢˜
- Theoretical Analysis is hard! because
  - Stochastic search algorithm	- Complex group dynamics	- Performance depends on the search landscape
    ä¼˜ç‚¹
- Regarding performance PSO is competitive to other known global optimization methods
